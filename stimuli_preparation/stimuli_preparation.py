# -*- coding: utf-8 -*-
"""stimuli_preparation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oy6CnlVkvtVzj1I8MlaDegmn_OI6wMbH

## Import
"""

!wget https://rusvectores.org/static/models/ruscorpora_upos_skipgram_300_10_2017.bin.gz
!wget http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz

import gensim
import random
import sklearn
import itertools
import numpy as np
import pandas as pd
from google.colab import files
from collections import Counter

m_2017 = 'ruscorpora_upos_skipgram_300_10_2017.bin.gz'
model_2017 = gensim.models.KeyedVectors.load_word2vec_format(m_2017, binary=True)
model_2017.init_sims(replace=True)

m_2015 = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'
model_2015 = gensim.models.KeyedVectors.load_word2vec_format(m_2015, binary=True)
model_2015.init_sims(replace=True)

"""## Choose AL and L1"""

def get_similarity_matrix(words, model=None):
  vecs = dict()
  for word in words:
    vecs[word] = {w: model.similarity(f'{word}_NOUN', f'{w}_NOUN') \
                 for w in words if w != word}
  return vecs


def get_scores(stimuli_list, save=False, model=model_2015):
    all_similarities = {}
    vecs = np.array([model[word+ '_S'] for word in stimuli_list])
    all_sim = {}
    for idx, word in enumerate(stimuli_list):
        sims = model.cosine_similarities(vecs[idx], vecs)
        all_sim[word] = sims
        ranged = np.argsort(sims, axis=-1)
        less_sim = [(stimuli_list[i], sims[i]) for i in ranged]
        all_similarities[word] = less_sim[:-1]
    sim_means = dict()
    for word in stimuli_list:
        my_array = np.array([i[1] for i in all_similarities[word]])
        sim_means[word] = (np.mean(my_array), max(my_array), np.std(my_array))
    all_scores = pd.DataFrame(stimuli_list, columns=['keys']).join(
                pd.DataFrame(all_sim))

    for word in all_similarities:
      all_similarities[word] = sorted(all_similarities[word], 
                                      key=lambda x: x[1], reverse=True)
    if save:
        all_scores.to_excel('sim_scores_all.xlsx')
        files.download('sim_scores_all.xlsx')
    return sim_means, all_similarities


def get_l1_list(file='L1_candidates.csv', save=False, 
                top_sim=0.47, model=model_2017):
    candidates = pd.read_csv(file, sep=';')
    words = candidates['dominant_name']
    _, scores = get_scores(words, model)
    all_s = {}
    for word in scores:
        all_s[word] = [x for x in scores[word] if x[1] > top_sim]
    raw = [x for x in all_s if all_s[x] == []]
    rich = [(x, all_s[x]) for x in all_s if all_s[x] != []]
    rich_set = []
    my_set = []
    for word in rich:
      rich_set += [x[0] for x in word[1]]
      if word[0] not in rich_set:
        my_set.append(word[0])

    _, scores  = get_scores(raw + my_set)
    all_s = {}
    for word in scores:
        all_s[word] = [x for x in scores[word] if x[1] > top_sim]
    raw_2 = [x for x in all_s if all_s[x] == []]

    e, _ = get_scores(raw_2)

    my_table = pd.DataFrame([(x, e[x][0], e[x][1], e[x][2]) for x in e], 
                            columns=['dominant_name', 'sim_score_mean', 
                                     'sim_score_max', 'sim_score_sd'])
    all_stim = pd.merge(candidates, my_table, on='dominant_name', how='right')
    if save:
      all_stim.to_excel('all_data.xlsx', index=False)
      files.download('all_data.xlsx')
    return all_stim

"""## Experimental lists

### L1
"""

def divide_l1(offset=2, file= 'l1.csv', model=None):
    mini_dict = {'самолет':'руль',
                 'руль':'самолет',
                 'корона':'костюм',
                 'костюм':'корона'}
    df = pd.read_csv(file, sep=';')
    words = [i.dominant_name for _, i in df.iterrows()]
    vecs = get_similarity_matrix(words, model)
    set_1 = []
    set_2 = []
    all_sets = list(vecs.keys())
    for i in range(40):
        random.seed(i*offset)
        word = random.choice(all_sets)
        most_sim = sorted(vecs[word].items(), 
                          key=lambda x: x[1], reverse=True)
        most_sim = [i for i in most_sim if i[0] in all_sets and i[0] != word]
        try:
          most_sim = most_sim[0][0]
          if (most_sim in list(mini_dict.keys()) \
              and mini_dict[most_sim] in set_2) or \
              (word in list(mini_dict.keys()) \
              and mini_dict[word] in set_1):
              most_sim, word = word, most_sim
          set_1.append(word)
          set_2.append(most_sim)
          all_sets.remove(word)
          all_sets.remove(most_sim)
        except Exception as e:
          print(e, most_sim, word)                
    return list(sorted(set_1)), list(sorted(set_2))

def l1_stats(set_1, set_2, file='l1.csv', save=False, model=None):
    with open(file, 'r') as f:
      l1_all = pd.read_csv(f, sep=';') 

    for idx, item in enumerate([set_1, set_2]):
        df = pd.DataFrame(item, columns = ['dominant_name'])
        tmp = pd.merge(l1_all, df, on=['dominant_name'])
        sims = dict()
        matrix = get_similarity_matrix(item)
        for word in item:
            sims[word] = np.mean(matrix[item])
        tmp['sim_score_mean'] = list(scores.values())
        for param in ['Frequency', 'Phonemes',
                      'Imageability_Mean', 
                      'Age_acquisition_Mean',
                      'Object_familiarity_Mean']:
            tmp[param] = [float(str(i).replace(',', '.')) for i in tmp[param]]
        print(tmp.mean(), '\n=============\n')

        if save:
            tmp.to_excel(f'l1_set_{idx+1}.xlsx', index=False)
            files.download(f'l1_set_{idx+1}.xlsx')

"""### AL"""

def divide_al(ww, file, offset=3, save=False):
    with open(file, 'r') as f:
        words = pd.read_csv(f, sep=';')[:80]

    set_1 = []
    set_2 = []
    u = [item for item in ww if item[-1] == 'u']
    e = [item for item in ww if item[-1] == 'e']
    for idx in range(40):
      random.seed(idx*offset)
      wu = random.choice(u)
      we = random.choice(e)
      if idx % 2 == 0:
        set_1 += [wu, we]
      else:
        set_2 += [wu, we]
      u.remove(wu)
      e.remove(we)

    for idx, al in enumerate([set_1, set_2]):  
      al = pd.merge(words, pd.DataFrame(al, columns=['word']), on=['word'])
      for param in ['mean_dist', 'mean_valency', 'sd_valency']:
          al[param] = [float(i.replace(',', '.')) for i in al[param]]
      if save:
        al.to_excel(f'al{idx+1}.xlsx')
        files.download(f'al{idx+1}.xlsx')
      
      yield al['word']

"""### AFC"""

def getafc(word_set, max_nb=4, model=None):
  two = list(iter(itertools.combinations(word_set, 2)))
  used = list()
  my_counter = Counter()
  for idx, word in enumerate(word_set):
    array = [i for i in two 
             if word in i 
             and i not in used 
             and my_counter[i[0]] <= max_nb
             and my_counter[i[1]] <= max_nb
             and model.similarity(f'{i[0]}_NOUN', f'{i[1]}_NOUN') < 0.3
             ]
    random.seed(idx)
    t = random.choice(array)
    my_counter[t[0]] += 1
    my_counter[t[1]] += 1
    used.append(t)

  wordset = word_set[:]
  pairs = [sorted(tuple(i)) for i in used]
  res = dict()
  for pair in pairs:
    head = {word: (model.similarity(f'{pair[0]}_NOUN', f'{word}_NOUN'),
                   model.similarity(f'{pair[1]}_NOUN', f'{word}_NOUN'))
            for word in wordset}
    tmp_head = {word: head[word] for word in head
            if head[word][0] < 0.3 and head[word][1] < 0.3
            and sorted(tuple([pair[0], word])) not in pairs
            and sorted(tuple([pair[1], word])) not in pairs}
    if len(tmp_head) == 0:
      tmp_head = {word: head[word] for word in head
            if sorted(tuple([pair[0], word])) not in pairs
            and sorted(tuple([pair[1], word])) not in pairs}
    head = sorted(tmp_head.items(), key = lambda x: np.sum(x[1]))[0][0]
    res[head] = pair
    wordset.remove(head)
  return res


def AFC(set_1, set_2, file='l1.csv', save=False, model=None):
  a = getafc(set_1, model=model)
  b = getafc(set_2, model=model)
  a['стрела'][0], a['колокол'][0] = a['колокол'][0], a['стрела'][0]
  a['ведро'], a['колокол'] = a['колокол'], a['ведро']
   
  with open(file, 'r') as f:
    df = pd.read_csv(f, sep=';')

  if save: 
      for idx, word_set in enumerate([a, b]):
        set1 = []
        for word in word_set:
          set1.append([
                      word, a[word][0], a[word][1], 
                      model.similarity(f'{word}_NOUN', f'{a[word][0]}_NOUN'),
                      model.similarity(f'{word}_NOUN', f'{a[word][0]}_NOUN'),
                      model.similarity(f'{a[word][0]}_NOUN', f'{a[word][1]}_NOUN')
                      ])

        set1 = pd.DataFrame(set1, columns=['dominant_name',
                                          'AFC1', 'AFC2',
                                          'chosen_distance_word_to_1', 
                                          'chosen_distance_word_to_2',
                                          'chosen_distance_1_to_2'])
    
        set1 = pd.merge(set1, df, on=['dominant_name'])
        set1.to_excel(f'afc{idx+1}.xlsx', index=False)
        files.download(f'afc{idx+1}.xlsx')
        afc_stats([a, b])

  return a, b


def afc_stats(afc):
  tables = []
  for idx in [0, 1]:
      one = Counter()
      two = Counter()

      for word in afc[idx]:
        one[afc[idx][word][0]] += 1
        one[afc[idx][word][1]] += 1
        two[tuple([word, afc[idx][word][1]])] += 1
        two[tuple([word, afc[idx][word][0]])] += 1
        two[tuple(afc[idx][word])] += 1

      A = pd.DataFrame([[i, one[i]] for i in one]) 
      B = pd.DataFrame([[f'{i[0]} {i[1]}', two[i]] for i in two])
      C = pd.concat([A, B], axis=0, join='outer')
      tables.append(C)
  D = pd.concat(tables, axis=1, join='outer')
  D.to_excel('afc_stats.xlsx',index=False)
  files.download('afc_stats.xlsx')
  return D

"""### Recognition"""

def recognition_lists(word_list, al, nbs, offset=15):
    words = word_list[:]
    r_keys = dict()
    for i in range(20):
        random.seed(i*offset)
        word = random.choice(words)
        r_keys[word] = word
        words.remove(word)
    distractors = words[:]

    for idx, word in enumerate(words):
        random.seed((idx+20)*offset)
        k = random.choice(distractors)
        r_keys[word] = k
        distractors.remove(k)
    recog_list = {w: r_keys[w] for w in word_list}
    recog_cor = [int(recog_list[s] == s) for s in recog_list]
    recog_list = [f'{s} = {recog_list[w]}?' for w, s in zip(recog_list, al)]
    return recog_list, recog_cor

"""### List creation"""

def random_numbers(save=False):
    """
    generates a unique L1 order for each experimental list
    """
    all_sets = []

    idx = 0
    a = list(range(40))
    while len(all_sets) < 200:
      tmp = list()
      while len(tmp) < 40:
        idx += 1
        random.seed(idx)
        q = random.choice(a)
        if q not in tmp:
          tmp.append(q)
      if tmp not in all_sets:
        all_sets.append(tmp)

    prev = []
    for idx, item in enumerate(all_sets[:]):
      if item[:4] in prev:
        all_sets.remove(item)
      else:
        prev.append(item[:4])

    if save:
        pd.DataFrame(all_sets).to_excel('numbers.xlsx', index=False)
        files.download('numbers.xlsx')

    return all_sets

def single_list(nbs, word_set, al, afc_dict, i):
    s1 = [word_set[idx] for idx in nbs]
    sound_paths = [f'{word}.wav' for word in al]
    afc = []
    for idx in range(40):
      word = s1[idx]
      random.seed(idx*15)
      w = random.sample([word, afc_dict[word][0], afc_dict[word][1]], 3)
      afc.append(w)
    afc0 = [w[0] for w in afc]
    afc1 = [w[1] for w in afc]
    afc2 = [w[2] for w in afc]
    afc0_color = [('black', 'green')[w[0] == s] for w, s in zip(afc, s1)]
    afc1_color = [('black', 'green')[w[1] == s] for w, s in zip(afc, s1)]
    afc2_color = [('black', 'green')[w[2] == s] for w, s in zip(afc, s1)]
    recog_list, recog_cor = recognition_lists(s1, al, nbs, offset=i)
    recall_al = [al[nb] for nb in nbs]
    recall_l1 = [s1[nb] for nb in nbs]
    sem_al = [recall_al[nb] for nb in nbs]
    table = list(zip(
        al, s1, sound_paths, 
        afc0, afc1, afc2, 
        afc0_color, afc1_color, afc2_color,
        recog_list, recog_cor,  
        recall_al, recall_l1,
        sem_al
        ))
    table = [table[idx] for idx in nbs]
    df = pd.DataFrame(table,
        columns = [
                   'AL', 'L1', 'sound_path', 
                   'AFC1', 'AFC2', 'AFC3',
                   'AFC1_color', 'AFC2_color', 'AFC3_color',
                   'recognition_text', 'recognition_correctness',
                   'recall_AL', 'recall_L1',
                   'SEM_AL'
        ])
    return df


def get_all(save=False, model=model_2017, list_nb=15):
    ww = pd.read_csv('al.csv', sep=';')['word'][:80]
    al = list(divide_al(ww, 'al.csv'))

    set_1, set_2 = divide_l1(file='l1.csv', offset=1, model=model)
    afc_1, afc_2 = AFC(set_1, set_2, model=model)

    numbers = random_numbers()
    for i in range(list_nb):
      df_1 = single_list(numbers[i], set_1, al[i % 2 == 1], afc_1, i)
      df_2 = single_list(numbers[i], set_2, al[i % 2 == 0], afc_2, i) 

      if save:
          df_1.to_excel(f'session_1_{i}.xlsx', index=False)
          df_2.to_excel(f'session_2_{i}.xlsx', index=False)
          files.download(f'session_1_{i}.xlsx')
          files.download(f'session_2_{i}.xlsx')
          
    yield df_1, df_2

"""## main"""

lists = get_all()